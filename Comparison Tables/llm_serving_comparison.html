<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Serving Frameworks Comparison</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Inter', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 40px 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
        }
        
        h1 {
            text-align: center;
            color: white;
            font-size: 2.5rem;
            margin-bottom: 10px;
            font-weight: 700;
            text-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .subtitle {
            text-align: center;
            color: rgba(255,255,255,0.9);
            font-size: 1.1rem;
            margin-bottom: 40px;
        }
        
        .table-wrapper {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.15);
            overflow: hidden;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
        }
        
        thead {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        
        thead th {
            padding: 20px 15px;
            text-align: left;
            color: white;
            font-weight: 600;
            font-size: 1.05rem;
            border-right: 1px solid rgba(255,255,255,0.1);
        }
        
        thead th:last-child {
            border-right: none;
        }
        
        thead th:first-child {
            background: rgba(0,0,0,0.1);
            font-weight: 700;
        }
        
        .framework-name {
            font-size: 1.15rem;
            letter-spacing: 0.5px;
        }
        
        tbody tr {
            border-bottom: 1px solid #e5e7eb;
            transition: background 0.2s;
        }
        
        tbody tr:hover {
            background: #f9fafb;
        }
        
        tbody tr:last-child {
            border-bottom: none;
        }
        
        tbody td {
            padding: 18px 15px;
            font-size: 0.95rem;
            line-height: 1.6;
            color: #374151;
            vertical-align: top;
        }
        
        tbody td:first-child {
            font-weight: 600;
            background: #f9fafb;
            color: #111827;
            border-right: 2px solid #e5e7eb;
            font-size: 0.9rem;
        }
        
        /* Highlight important metrics */
        .metric {
            background: #fef3c7;
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 600;
            color: #92400e;
            display: inline-block;
            margin: 2px 0;
        }
        
        .performance {
            background: #dcfce7;
            color: #166534;
        }
        
        .key-feature {
            font-weight: 600;
            color: #6366f1;
        }
        
        /* Responsive design */
        @media (max-width: 1200px) {
            table {
                font-size: 0.9rem;
            }
            
            thead th {
                padding: 15px 10px;
                font-size: 0.95rem;
            }
            
            tbody td {
                padding: 14px 10px;
                font-size: 0.85rem;
            }
        }
        
        /* Custom scrollbar */
        .table-wrapper {
            overflow-x: auto;
        }
        
        .table-wrapper::-webkit-scrollbar {
            height: 8px;
        }
        
        .table-wrapper::-webkit-scrollbar-track {
            background: #f1f1f1;
        }
        
        .table-wrapper::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 4px;
        }
        
        .table-wrapper::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
        
        /* Footer */
        .footer {
            text-align: center;
            margin-top: 30px;
            color: rgba(255,255,255,0.8);
            font-size: 0.9rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>LLM Serving Frameworks Comparison</h1>
        <p class="subtitle">High-Performance Inference Solutions for Large Language Models</p>
        
        <div class="table-wrapper">
            <table>
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th><span class="framework-name">vLLM</span></th>
                        <th><span class="framework-name">TensorRT-LLM</span></th>
                        <th><span class="framework-name">HF TGI v3</span></th>
                        <th><span class="framework-name">LMDeploy</span></th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Primary Focus</td>
                        <td>Open, high throughput serving with <span class="key-feature">PagedAttention</span> and OpenAI style API</td>
                        <td>Maximize throughput and TTFT on NVIDIA GPUs with custom kernels and quantization</td>
                        <td>Long prompt efficiency, multi backend serving, production gateway</td>
                        <td>Efficient open model serving with persistent batch, blocked KV and quantization</td>
                    </tr>
                    <tr>
                        <td>Core KV/Attention</td>
                        <td><span class="key-feature">PagedAttention</span>, paged KV blocks, near zero fragmentation</td>
                        <td>Paged attention, quantized KV cache, CUDA graphs, speculative decoding</td>
                        <td>Paged attention + <span class="key-feature">chunking</span> and <span class="key-feature">prefix cache</span> for long contexts</td>
                        <td><span class="key-feature">Blocked KV cache</span>, persistent batch, dynamic split/fuse, KV quantization</td>
                    </tr>
                    <tr>
                        <td>Throughput vs Baselines</td>
                        <td><span class="metric performance">2–4×</span> higher than FasterTransformer/Orca</td>
                        <td>H100 FP8: <span class="metric performance">&gt;10k tok/s</span> at ≈100ms TTFT, up to <span class="metric performance">4.6×</span> A100 throughput</td>
                        <td>Similar to vLLM on short prompts; large gains on long context via caching</td>
                        <td><span class="metric performance">Up to 1.8×</span> higher request throughput than vLLM on supported models</td>
                    </tr>
                    <tr>
                        <td>Long Prompt Performance<br>(200k+ tokens)</td>
                        <td>Baseline, recomputes full prefill for each turn</td>
                        <td>Fast single pass prefill, no built-in conversation prefix cache</td>
                        <td><span class="metric performance">13× speedup vs vLLM</span> (27.5s → 2s) via prefix caching, ≈3× more tokens/GPU</td>
                        <td>Efficient single pass with blocked KV; no documented 13× style prefix cache benchmark</td>
                    </tr>
                    <tr>
                        <td>Batch/Scheduler</td>
                        <td>Continuous batching, good scaling until KV/compute saturation</td>
                        <td>Inflight batching, CUDA graphs; needs tuned batch sizes</td>
                        <td>Continuous batching + chunking; stable under high concurrency</td>
                        <td>Persistent batch (continuous batching) as core feature</td>
                    </tr>
                    <tr>
                        <td>Latency (TTFT/P50/P99)</td>
                        <td>Low P50 under moderate load; P99 sensitive to KV pressure</td>
                        <td>H100 FP8: <span class="metric">≈100ms TTFT</span> at peak, <span class="metric performance">&lt;10ms TTFT</span> in min latency mode</td>
                        <td>Large drop in P50/P99 vs vLLM for long prompts (avoids repeated prefill)</td>
                        <td>Competitive TTFT; improved under quantization (<span class="metric performance">up to 2.4×</span> vs FP16)</td>
                    </tr>
                    <tr>
                        <td>Multi-Model/Tenant</td>
                        <td>One model per process; multi-tenant via external router or Ray/K8s</td>
                        <td>Multi-tenant/model at Triton/Ray or custom orchestrator layer</td>
                        <td>Router + model server design, multi-backend/model aware out of box</td>
                        <td>Proxy server supports multi-model, multi-machine, multi-GPU natively</td>
                    </tr>
                    <tr>
                        <td>Relative Cost per 1M Tokens</td>
                        <td><span class="metric">Baseline</span></td>
                        <td>Lower than vLLM when tuned (higher tokens/s on NVIDIA)</td>
                        <td>Similar on short prompts, <span class="metric performance">order of magnitude cheaper</span> on long multi-turn (cached prefill)</td>
                        <td>Cheaper than vLLM (<span class="metric performance">~0.55×</span> cost if 1.8× throughput)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="footer">
            <p>Performance metrics vary based on hardware, model size, and workload characteristics</p>
        </div>
    </div>
</body>
</html>